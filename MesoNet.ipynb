{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import Model,Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 10\n",
    "max_df = 2\n",
    "window_size=7\n",
    "supersteps = (max_frames-window_size+1)\n",
    "\n",
    "df_train0 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_0/metadata.json')\n",
    "df_train1 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_1/metadata.json')\n",
    "df_train2 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_2/metadata.json')\n",
    "df_train3 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_3/metadata.json')\n",
    "df_train4 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_4/metadata.json')\n",
    "df_train5 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_5/metadata.json')\n",
    "df_train6 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_6/metadata.json')\n",
    "df_train7 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_7/metadata.json')\n",
    "df_train8 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_8/metadata.json')\n",
    "df_train9 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_9/metadata.json')\n",
    "df_train10 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_10/metadata.json')\n",
    "df_train11 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_11/metadata.json')\n",
    "df_train12 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_12/metadata.json')\n",
    "df_train13 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_13/metadata.json')\n",
    "df_train14 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_14/metadata.json')\n",
    "df_train15 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_15/metadata.json')\n",
    "df_train16 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_16/metadata.json')\n",
    "df_train17 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_17/metadata.json')\n",
    "df_train18 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_18/metadata.json')\n",
    "df_train19 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_19/metadata.json')\n",
    "df_train20 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_20/metadata.json')\n",
    "df_train21 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_21/metadata.json')\n",
    "df_train22 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_22/metadata.json')\n",
    "df_train23 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_23/metadata.json')\n",
    "df_train24 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_24/metadata.json')\n",
    "df_train25 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_25/metadata.json')\n",
    "df_train26 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_26/metadata.json')\n",
    "df_train27 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_27/metadata.json')\n",
    "df_train28 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_28/metadata.json')\n",
    "df_train29 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_29/metadata.json')\n",
    "df_train30 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_30/metadata.json')\n",
    "df_train31 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_31/metadata.json')\n",
    "df_train32 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_32/metadata.json')\n",
    "df_train33 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_33/metadata.json')\n",
    "df_train34 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_34/metadata.json')\n",
    "df_train35 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_35/metadata.json')\n",
    "df_train36 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_36/metadata.json')\n",
    "df_train37 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_37/metadata.json')\n",
    "df_train38 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_38/metadata.json')\n",
    "df_train39 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_39/metadata.json')\n",
    "df_train40 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_40/metadata.json')\n",
    "df_train41 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_41/metadata.json')\n",
    "df_train42 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_42/metadata.json')\n",
    "df_train43 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_43/metadata.json')\n",
    "df_train44 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_44/metadata.json')\n",
    "df_train45 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_45/metadata.json')\n",
    "df_train46 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_46/metadata.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_val1 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_47/metadata.json')\n",
    "df_val2 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_48/metadata.json')\n",
    "df_val3 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_49/metadata.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_trains = [df_train0 ,df_train1, df_train2, df_train3, df_train4,\n",
    "             df_train5, df_train6, df_train7, df_train8, df_train9,df_train10,\n",
    "            df_train11, df_train12, df_train13, df_train14, df_train15,df_train16, \n",
    "            df_train17, df_train18, df_train19, df_train20, df_train21, df_train22, \n",
    "            df_train23, df_train24, df_train25, df_train26, df_train27, df_train28, \n",
    "            df_train29, df_train30, df_train31, df_train32, df_train33, df_train34,\n",
    "            df_train34, df_train35, df_train36, df_train37, df_train38, df_train39,\n",
    "            df_train40, df_train41, df_train42, df_train43, df_train44, df_train45,\n",
    "            df_train46]\n",
    "\n",
    "df_vals=[df_val1, df_val2, df_val3]\n",
    "nums = list(range(len(df_trains)+1))\n",
    "LABELS = ['REAL','FAKE']\n",
    "val_nums=[47, 48, 49]\n",
    "frame_index = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:17<00:00,  2.77it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 19.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_path(num,x):\n",
    "    num=str(num)\n",
    "    if len(num)==2:\n",
    "        path='/home/aelbakry1999/images/margin_0/dfdc_train_part_'+ num +'/'+ x.replace('.mp4', '') + '/frame' + str(frame_index) +'.jpeg'\n",
    "    else:\n",
    "        path='/home/aelbakry1999/images/margin_0/dfdc_train_part_'+ num +'/'+ x.replace('.mp4', '') + '/frame' + str(frame_index) +'.jpeg'\n",
    "    if not os.path.exists(path):\n",
    "       raise Exception\n",
    "    return path\n",
    "paths=[]\n",
    "y=[]\n",
    "for df_train,num in tqdm(zip(df_trains,nums),total=len(df_trains)):\n",
    "    images = list(df_train.columns.values)\n",
    "    for x in images:\n",
    "        try:\n",
    "            paths.append(get_path(num,x))\n",
    "            y.append(LABELS.index(df_train[x]['label']))\n",
    "        except Exception as err:\n",
    "            #print(err)\n",
    "            pass\n",
    "\n",
    "val_paths=[]\n",
    "val_y=[]\n",
    "for df_val,num in tqdm(zip(df_vals,val_nums),total=len(df_vals)):\n",
    "    images = list(df_val.columns.values)\n",
    "    for x in images:\n",
    "        try:\n",
    "            val_paths.append(get_path(num,x))\n",
    "            val_y.append(LABELS.index(df_val[x]['label']))\n",
    "        except Exception as err:\n",
    "            #print(err)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65174 fake train samples\n",
      "There are 12109 real train samples\n",
      "There are 6216 fake val samples\n",
      "There are 1290 real val samples\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(y.count(1))+' fake train samples')\n",
    "print('There are '+str(y.count(0))+' real train samples')\n",
    "print('There are '+str(val_y.count(1))+' fake val samples')\n",
    "print('There are '+str(val_y.count(0))+' real val samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "real=[]\n",
    "fake=[]\n",
    "for m,n in zip(paths,y):\n",
    "    if n==0:\n",
    "        real.append(m)\n",
    "    else:\n",
    "        fake.append(m)\n",
    "fake=random.sample(fake,len(real))\n",
    "paths,y=[],[]\n",
    "for x in real:\n",
    "    paths.append(x)\n",
    "    y.append(0)\n",
    "for x in fake:\n",
    "    paths.append(x)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=[]\n",
    "fake=[]\n",
    "for m,n in zip(val_paths,val_y):\n",
    "    if n==0:\n",
    "        real.append(m)\n",
    "    else:\n",
    "        fake.append(m)\n",
    "fake=random.sample(fake,len(real))\n",
    "val_paths,val_y=[],[]\n",
    "for x in real:\n",
    "    val_paths.append(x)\n",
    "    val_y.append(0)\n",
    "for x in fake:\n",
    "    val_paths.append(x)\n",
    "    val_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12109 fake train samples\n",
      "There are 12109 real train samples\n",
      "There are 1290 fake val samples\n",
      "There are 1290 real val samples\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(y.count(1))+' fake train samples')\n",
    "print('There are '+str(y.count(0))+' real train samples')\n",
    "print('There are '+str(val_y.count(1))+' fake val samples')\n",
    "print('There are '+str(val_y.count(0))+' real val samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24218/24218 [00:22<00:00, 1075.16it/s]\n",
      "100%|██████████| 2580/2580 [00:02<00:00, 1283.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    return cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "X=[]\n",
    "for img in tqdm(paths):\n",
    "    X.append(read_img(img))\n",
    "val_X=[]\n",
    "for img in tqdm(val_paths):\n",
    "    val_X.append(read_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def shuffle(X,y):\n",
    "    new_train=[]\n",
    "    for m,n in zip(X,y):\n",
    "        new_train.append([m,n])\n",
    "    random.shuffle(new_train)\n",
    "    X,y=[],[]\n",
    "    for x in new_train:\n",
    "        X.append(x[0])\n",
    "        y.append(x[1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=shuffle(X,y)\n",
    "val_X,val_y=shuffle(val_X,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionLayer(a, b, c, d):\n",
    "    def func(x):\n",
    "        x1 = Conv2D(a, (1, 1), padding='same', activation='elu')(x)\n",
    "        \n",
    "        x2 = Conv2D(b, (1, 1), padding='same', activation='elu')(x)\n",
    "        x2 = Conv2D(b, (3, 3), padding='same', activation='elu')(x2)\n",
    "            \n",
    "        x3 = Conv2D(c, (1, 1), padding='same', activation='elu')(x)\n",
    "        x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides = 1, padding='same', activation='elu')(x3)\n",
    "        \n",
    "        x4 = Conv2D(d, (1, 1), padding='same', activation='elu')(x)\n",
    "        x4 = Conv2D(d, (3, 3), dilation_rate = 3, strides = 1, padding='same', activation='elu')(x4)\n",
    "        y = Concatenate(axis = -1)([x1, x2, x3, x4])\n",
    "            \n",
    "        return y\n",
    "    return func\n",
    "    \n",
    "def define_model(shape=(256,256,3)):\n",
    "    x = Input(shape = shape)\n",
    "    \n",
    "    x1 = InceptionLayer(1, 4, 4, 2)(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "    \n",
    "    x2 = InceptionLayer(2, 4, 4, 2)(x1)\n",
    "    x2 = BatchNormalization()(x2)        \n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "        \n",
    "    x3 = Conv2D(16, (5, 5), padding='same', activation = 'elu')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "    x4 = Conv2D(16, (5, 5), padding='same', activation = 'elu')(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    if shape==(256,256,3):\n",
    "        x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "    else:\n",
    "        x4 = MaxPooling2D(pool_size=(2, 2), padding='same')(x4)\n",
    "    y = Flatten()(x4)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(16)(y)\n",
    "    y = LeakyReLU(alpha=0.1)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1, activation = 'sigmoid')(y)\n",
    "    model=Model(inputs = x, outputs = y)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4))\n",
    "    #model.summary()\n",
    "    return model\n",
    "df_model=define_model()\n",
    "df_model.load_weights('/home/aelbakry1999/meso-pretrain/MesoInception_DF')\n",
    "f2f_model=define_model()\n",
    "f2f_model.load_weights('/home/aelbakry1999/meso-pretrain/MesoInception_F2F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "lrs=[1e-3,5e-4,1e-4]\n",
    "def schedule(epoch):\n",
    "    return lrs[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRETRAIN=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 160, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 160, 160, 4)  16          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 160, 160, 4)  16          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 160, 160, 2)  8           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 160, 160, 1)  4           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 160, 160, 4)  148         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 160, 160, 4)  148         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 160, 160, 2)  38          conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 160, 160, 11) 0           conv2d_49[0][0]                  \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 160, 160, 11) 44          concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 80, 80, 11)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 80, 80, 4)    48          max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 80, 80, 4)    48          max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 80, 80, 2)    24          max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 80, 80, 2)    24          max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 80, 80, 4)    148         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 80, 80, 4)    148         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 80, 80, 2)    38          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 80, 80, 12)   0           conv2d_56[0][0]                  \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "                                                                 conv2d_60[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 80, 80, 12)   48          concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 40, 40, 12)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 40, 40, 16)   4816        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 40, 40, 16)   64          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 20, 20, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 20, 20, 16)   6416        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 20, 16)   64          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 10, 10, 16)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1600)         0           max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1600)         0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           25616       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 16)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16)           0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            17          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 37,941\n",
      "Trainable params: 37,831\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.7023\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 42s 2ms/step - loss: 0.6460\n",
      "fold 0 model loss: 0.6205293461369972\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 45s 2ms/step - loss: 0.7301\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 43s 2ms/step - loss: 0.6496\n",
      "fold 1 model loss: 0.6366484126725862\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 46s 2ms/step - loss: 0.7119\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6472\n",
      "fold 2 model loss: 0.6432976727915365\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 45s 2ms/step - loss: 0.7044\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 43s 2ms/step - loss: 0.6397\n",
      "fold 3 model loss: 0.6275804243891594\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 46s 2ms/step - loss: 0.6996\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6403\n",
      "fold 4 model loss: 0.6260192608785664\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 45s 2ms/step - loss: 0.7217\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6422\n",
      "fold 0 model loss: 0.6356279384662477\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.7393\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 42s 2ms/step - loss: 0.6550\n",
      "fold 1 model loss: 0.6248937905535044\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 45s 2ms/step - loss: 0.7397\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6589\n",
      "fold 2 model loss: 0.654417010168581\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 45s 2ms/step - loss: 0.7422\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6475\n",
      "fold 3 model loss: 0.630628743125198\n",
      "Epoch 1/2\n",
      "24218/24218 [==============================] - 46s 2ms/step - loss: 0.7144\n",
      "Epoch 2/2\n",
      "24218/24218 [==============================] - 44s 2ms/step - loss: 0.6469\n",
      "fold 4 model loss: 0.627774658189841\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "kfolds=5\n",
    "losses=[]\n",
    "if LOAD_PRETRAIN:\n",
    "    # import keras.backend as K\n",
    "    df_models=[]\n",
    "    f2f_models=[]\n",
    "    i=0\n",
    "    while len(df_models)<kfolds:\n",
    "        model=define_model((160,160,3))\n",
    "        if i==0:\n",
    "            model.summary()\n",
    "        #model.load_weights('../input/meso-pretrain/MesoInception_DF')\n",
    "        for new_layer, layer in zip(model.layers[1:-8], df_model.layers[1:-8]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "        model.fit([X],[y],epochs=2,callbacks=[LearningRateScheduler(schedule)])\n",
    "        pred=model.predict([val_X])\n",
    "        loss=log_loss(val_y,pred)\n",
    "        losses.append(loss)\n",
    "        print('fold '+str(i)+' model loss: '+str(loss))\n",
    "        df_models.append(model)\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        i+=1\n",
    "    i=0\n",
    "    while len(f2f_models)<kfolds:\n",
    "        model=define_model((160,160,3))\n",
    "        #model.load_weights('../input/meso-pretrain/MesoInception_DF')\n",
    "        for new_layer, layer in zip(model.layers[1:-8], f2f_model.layers[1:-8]):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "        model.fit([X],[y],epochs=2,callbacks=[LearningRateScheduler(schedule)])\n",
    "        pred=model.predict([val_X])\n",
    "        loss=log_loss(val_y,pred)\n",
    "        losses.append(loss)\n",
    "        print('fold '+str(i)+' model loss: '+str(loss))\n",
    "        f2f_models.append(model)\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        i+=1\n",
    "        models=f2f_models+df_models\n",
    "else:\n",
    "    models=[]\n",
    "    i=0\n",
    "    while len(models)<kfolds:\n",
    "        model=define_model((160,160,3))\n",
    "        if i==0:\n",
    "            model.summary()\n",
    "        model.fit([X],[y],epochs=2,callbacks=[LearningRateScheduler(schedule)])\n",
    "        pred=model.predict([val_X])\n",
    "        loss=log_loss(val_y,pred)\n",
    "        losses.append(loss)\n",
    "        print('fold '+str(i)+' model loss: '+str(loss))\n",
    "        if loss<0.68:\n",
    "            models.append(model)\n",
    "        else:\n",
    "            print('loss too bad, retrain!')\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1443e553fc4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2f_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mnew_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_pipline(X,two_times=False):\n",
    "    preds=[]\n",
    "    for model in tqdm(models):\n",
    "        pred=model.predict([X])\n",
    "        preds.append(pred)\n",
    "    preds=sum(preds)/len(preds)\n",
    "    if two_times:\n",
    "        return larger_range(preds,2)\n",
    "    else:\n",
    "        return preds\n",
    "def larger_range(model_pred,time):\n",
    "    return (((model_pred-0.5)*time)+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a78b0543fdd42ea8d98e3bccdd22942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_pred=models[losses.index(min(losses))].predict([val_X])\n",
    "model_pred=prediction_pipline(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(val_y, model_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[624, 666],\n",
       "       [305, 985]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Confusion Matrix -------------- \n",
      "[[624 666]\n",
      " [305 985]]\n",
      "True Positives: 985, True Negatives: 624, False Positives: 666, False Negatives: 305\n",
      "-------------- Model Scores -------------- \n",
      "Precision: 0.5966081162931557, Accuracy: 0.6236434108527131, Recall: 0.7635658914728682, F1-score: 0.6698401904114247\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(val_y, model_pred.round()).ravel()\n",
    "\n",
    "\n",
    "print(\"-------------- Confusion Matrix -------------- \")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "print('True Positives: {}, True Negatives: {}, False Positives: {}, False Negatives: {}'.format(tp, tn, fp, fn))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*(precision*recall)/ (precision+recall)\n",
    "print(\"-------------- Model Scores -------------- \")\n",
    "print('Precision: {}, Accuracy: {}, Recall: {}, F1-score: {}'.format(precision, accuracy, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct ✅  prediction: 0.5685955, answer: 1\n",
      "correct ✅  prediction: 0.6824382, answer: 1\n",
      "correct ✅  prediction: 0.5876818, answer: 1\n",
      "incorrect❌ prediction: 0.50406164, answer: 0\n",
      "incorrect❌ prediction: 0.39997956, answer: 1\n",
      "correct ✅  prediction: 0.6300886, answer: 1\n",
      "incorrect❌ prediction: 0.73205143, answer: 0\n",
      "correct ✅  prediction: 0.4922247, answer: 0\n",
      "correct ✅  prediction: 0.7018776, answer: 1\n",
      "correct ✅  prediction: 0.41646662, answer: 0\n",
      "correct ✅  prediction: 0.4985947, answer: 0\n",
      "correct ✅  prediction: 0.57241046, answer: 1\n",
      "correct ✅  prediction: 0.4445768, answer: 0\n",
      "correct ✅  prediction: 0.2234823, answer: 0\n",
      "correct ✅  prediction: 0.6277607, answer: 1\n",
      "correct ✅  prediction: 0.61718863, answer: 1\n",
      "correct ✅  prediction: 0.29933766, answer: 0\n",
      "number correct: 1673, number incorrect: 907\n",
      "64.8% correct, 35.2% incorrect\n"
     ]
    }
   ],
   "source": [
    "def check_answers(pred,real,num):\n",
    "    for i,(x,y) in enumerate(zip(pred,real)):\n",
    "        correct_incorrect='correct ✅ ' if round(float(x),0)==round(float(y),0) else 'incorrect❌'\n",
    "        print(correct_incorrect+' prediction: '+str(x[0])+', answer: '+str(y))\n",
    "        if i>num:\n",
    "            return\n",
    "def correct_precentile(pred,real):\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for x,y in zip(pred,real):\n",
    "        if round(float(x),0)==round(float(y),0):\n",
    "            correct+=1\n",
    "        else:\n",
    "            incorrect+=1\n",
    "    print('number correct: '+str(correct)+', number incorrect: '+str(incorrect))\n",
    "    print(str(round(correct/len(real)*100,1))+'% correct'+', '+str(round(incorrect/len(real)*100,1))+'% incorrect')\n",
    "check_answers(model_pred,val_y,15)\n",
    "correct_precentile(model_pred,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
