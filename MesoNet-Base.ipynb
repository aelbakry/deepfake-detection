{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import Model,Sequential\n",
    "# from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm,trange\n",
    "import glob\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train0 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_0/metadata.json')\n",
    "df_train1 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_1/metadata.json')\n",
    "df_train2 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_2/metadata.json')\n",
    "df_train3 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_3/metadata.json')\n",
    "df_train4 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_4/metadata.json')\n",
    "df_train5 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_5/metadata.json')\n",
    "df_train6 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_6/metadata.json')\n",
    "df_train7 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_7/metadata.json')\n",
    "df_train8 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_8/metadata.json')\n",
    "df_train9 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_9/metadata.json')\n",
    "df_train10 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_10/metadata.json')\n",
    "df_train11 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_11/metadata.json')\n",
    "df_train12 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_12/metadata.json')\n",
    "df_train13 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_13/metadata.json')\n",
    "df_train14 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_14/metadata.json')\n",
    "df_train15 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_15/metadata.json')\n",
    "df_train16 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_16/metadata.json')\n",
    "df_train17 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_17/metadata.json')\n",
    "df_train18 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_18/metadata.json')\n",
    "df_train19 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_19/metadata.json')\n",
    "df_train20 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_20/metadata.json')\n",
    "df_train21 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_21/metadata.json')\n",
    "df_train22 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_22/metadata.json')\n",
    "df_train23 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_23/metadata.json')\n",
    "df_train24 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_24/metadata.json')\n",
    "df_train25 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_25/metadata.json')\n",
    "df_train26 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_26/metadata.json')\n",
    "df_train27 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_27/metadata.json')\n",
    "df_train28 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_28/metadata.json')\n",
    "df_train29 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_29/metadata.json')\n",
    "df_train30 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_30/metadata.json')\n",
    "df_train31 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_31/metadata.json')\n",
    "df_train32 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_32/metadata.json')\n",
    "df_train33 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_33/metadata.json')\n",
    "df_train34 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_34/metadata.json')\n",
    "df_train35 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_35/metadata.json')\n",
    "df_train36 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_36/metadata.json')\n",
    "df_train37 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_37/metadata.json')\n",
    "df_train38 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_38/metadata.json')\n",
    "df_train39 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_39/metadata.json')\n",
    "df_train40 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_40/metadata.json')\n",
    "df_train41 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_41/metadata.json')\n",
    "df_train42 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_42/metadata.json')\n",
    "df_train43 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_43/metadata.json')\n",
    "df_train44 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_44/metadata.json')\n",
    "df_train45 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_45/metadata.json')\n",
    "df_train46 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_46/metadata.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_val1 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_47/metadata.json')\n",
    "df_val2 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_48/metadata.json')\n",
    "df_val3 = pd.read_json('/home/aelbakry1999/dfdc/dfdc_train_part_49/metadata.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_trains = [df_train0 ,df_train1, df_train2, df_train3, df_train4,\n",
    "             df_train5, df_train6, df_train7, df_train8, df_train9,df_train10,\n",
    "            df_train11, df_train12, df_train13, df_train14, df_train15,df_train16, \n",
    "            df_train17, df_train18, df_train19, df_train20, df_train21, df_train22, \n",
    "            df_train23, df_train24, df_train25, df_train26, df_train27, df_train28, \n",
    "            df_train29, df_train30, df_train31, df_train32, df_train33, df_train34,\n",
    "            df_train34, df_train35, df_train36, df_train37, df_train38, df_train39,\n",
    "            df_train40, df_train41, df_train42, df_train43, df_train44, df_train45,\n",
    "            df_train46]\n",
    "\n",
    "df_vals=[df_val1, df_val2, df_val3]\n",
    "nums = list(range(len(df_trains)+1))\n",
    "LABELS = ['REAL','FAKE']\n",
    "val_nums=[47, 48, 49]\n",
    "frame_index = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:35<00:00,  1.35it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_path(num,x):\n",
    "    num=str(num)\n",
    "    if len(num)==2:\n",
    "        path='/home/aelbakry1999/images/margin_0/dfdc_train_part_'+ num +'/'+ x.replace('.mp4', '') + '/frame' + str(frame_index) +'.jpeg'\n",
    "    else:\n",
    "        path='/home/aelbakry1999/images/margin_0/dfdc_train_part_'+ num +'/'+ x.replace('.mp4', '') + '/frame' + str(frame_index) +'.jpeg'\n",
    "    if not os.path.exists(path):\n",
    "       raise Exception\n",
    "    return path\n",
    "paths=[]\n",
    "y=[]\n",
    "for df_train,num in tqdm(zip(df_trains,nums),total=len(df_trains)):\n",
    "    images = list(df_train.columns.values)\n",
    "    for x in images:\n",
    "        try:\n",
    "            paths.append(get_path(num,x))\n",
    "            y.append(LABELS.index(df_train[x]['label']))\n",
    "        except Exception as err:\n",
    "            #print(err)\n",
    "            pass\n",
    "\n",
    "val_paths=[]\n",
    "val_y=[]\n",
    "for df_val,num in tqdm(zip(df_vals,val_nums),total=len(df_vals)):\n",
    "    images = list(df_val.columns.values)\n",
    "    for x in images:\n",
    "        try:\n",
    "            val_paths.append(get_path(num,x))\n",
    "            val_y.append(LABELS.index(df_val[x]['label']))\n",
    "        except Exception as err:\n",
    "            #print(err)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_train = y.count(1)\n",
    "lower_bound_train = y.count(0)\n",
    "\n",
    "upper_bound_val = val_y.count(1)\n",
    "lower_bound_val = val_y.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64011 fake train samples\n",
      "There are 11850 real train samples\n",
      "There are 6249 fake val samples\n",
      "There are 1276 real val samples\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(y.count(1))+' fake train samples')\n",
    "print('There are '+str(y.count(0))+' real train samples')\n",
    "print('There are '+str(val_y.count(1))+' fake val samples')\n",
    "print('There are '+str(val_y.count(0))+' real val samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    return cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def shuffle(X,y):\n",
    "    new_train=[]\n",
    "    for m,n in zip(X,y):\n",
    "        new_train.append([m,n])\n",
    "    random.shuffle(new_train)\n",
    "    X,y=[],[]\n",
    "    for x in new_train:\n",
    "        X.append(x[0])\n",
    "        y.append(x[1])\n",
    "    return X,y\n",
    "\n",
    "import random\n",
    "def get_random_sampling(paths, y, val_paths, val_y):\n",
    "  real=[]\n",
    "  fake=[]\n",
    "  for m,n in zip(paths,y):\n",
    "      if n==0:\n",
    "          real.append(m)\n",
    "      else:\n",
    "          fake.append(m)\n",
    "  # fake=random.sample(fake,len(real))\n",
    "  paths,y=[],[]\n",
    "  for x in real:\n",
    "      paths.append(x)\n",
    "      y.append(0)\n",
    "  for x in fake:\n",
    "      paths.append(x)\n",
    "      y.append(1)\n",
    "\n",
    "  real=[]\n",
    "  fake=[]\n",
    "  for m,n in zip(val_paths,val_y):\n",
    "      if n==0:\n",
    "          real.append(m)\n",
    "      else:\n",
    "          fake.append(m)\n",
    "  # fake=random.sample(fake,len(real))\n",
    "  val_paths,val_y=[],[]\n",
    "  for x in real:\n",
    "      val_paths.append(x)\n",
    "      val_y.append(0)\n",
    "  for x in fake:\n",
    "      val_paths.append(x)\n",
    "      val_y.append(1)\n",
    "\n",
    "  X=[]\n",
    "  for img in tqdm(paths):\n",
    "      X.append(read_img(img))\n",
    "  val_X=[]\n",
    "  for img in tqdm(val_paths):\n",
    "      val_X.append(read_img(img))\n",
    "\n",
    "  # Balance with ffhq dataset\n",
    "  ffhq = os.listdir('/home/aelbakry1999/FFHQ/thumbnails128x128')\n",
    "  X_ = []\n",
    "  for file in tqdm(ffhq):\n",
    "    im = read_img(f'/home/aelbakry1999/FFHQ/thumbnails128x128/{file}')\n",
    "    im = cv2.resize(im, (160,160))\n",
    "    X_.append(im)\n",
    "  random.shuffle(X_)\n",
    "\n",
    "  for i in range(upper_bound_train - lower_bound_train):\n",
    "    X.append(X_[i])\n",
    "    y.append(0)\n",
    "  del X_[0:upper_bound_train - lower_bound_train]\n",
    "  for i in range(upper_bound_val - lower_bound_val):\n",
    "    val_X.append(X_[i])\n",
    "    val_y.append(0)\n",
    "\n",
    "  X, y = shuffle(X,y)\n",
    "  val_X, val_y = shuffle(val_X,val_y)\n",
    "\n",
    "  return X, val_X, y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75861/75861 [01:22<00:00, 918.02it/s] \n",
      "100%|██████████| 7525/7525 [00:07<00:00, 963.01it/s] \n",
      "100%|██████████| 70000/70000 [00:47<00:00, 1467.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X, val_X, y, val_y = get_random_sampling(paths, y, val_paths, val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64011 fake train samples\n",
      "There are 64011 real train samples\n",
      "There are 6249 fake val samples\n",
      "There are 6249 real val samples\n"
     ]
    }
   ],
   "source": [
    "print('There are '+str(y.count(1))+' fake train samples')\n",
    "print('There are '+str(y.count(0))+' real train samples')\n",
    "print('There are '+str(val_y.count(1))+' fake val samples')\n",
    "print('There are '+str(val_y.count(0))+' real val samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionLayer(a, b, c, d):\n",
    "    def func(x):\n",
    "        x1 = tensorflow.keras.layers.Conv2D(a, (1, 1), padding='same', activation='elu')(x)\n",
    "        \n",
    "        x2 = tf.keras.layers.Conv2D(b, (1, 1), padding='same', activation='elu')(x)\n",
    "        x2 = tf.keras.layers.Conv2D(b, (3, 3), padding='same', activation='elu')(x2)\n",
    "            \n",
    "        x3 = tf.keras.layers.Conv2D(c, (1, 1), padding='same', activation='elu')(x)\n",
    "        x3 = tf.keras.layers.Conv2D(c, (3, 3), dilation_rate = 2, strides = 1, padding='same', activation='elu')(x3)\n",
    "        \n",
    "        x4 = tf.keras.layers.Conv2D(d, (1, 1), padding='same', activation='elu')(x)\n",
    "        x4 = tf.keras.layers.Conv2D(d, (3, 3), dilation_rate = 3, strides = 1, padding='same', activation='elu')(x4)\n",
    "        y = tf.keras.layers.Concatenate(axis = -1)([x1, x2, x3, x4])\n",
    "            \n",
    "        return y\n",
    "    return func\n",
    "    \n",
    "def define_model(shape=(256,256,3)):\n",
    "    x = tf.keras.layers.Input(shape = shape)\n",
    "    \n",
    "    x1 = InceptionLayer(1, 4, 4, 2)(x)\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "    \n",
    "    x2 = InceptionLayer(2, 4, 4, 2)(x1)\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2)        \n",
    "    x2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "        \n",
    "    x3 = tf.keras.layers.Conv2D(16, (5, 5), padding='same', activation = 'elu')(x2)\n",
    "    x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "    x4 = tf.keras.layers.Conv2D(16, (5, 5), padding='same', activation = 'elu')(x3)\n",
    "    x4 = tf.keras.layers.BatchNormalization()(x4)\n",
    "    if shape==(256,256,3):\n",
    "        x4 = tf.keras.layers.MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "    else:\n",
    "        x4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same')(x4)\n",
    "    y = tf.keras.layers.Flatten()(x4)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(16)(y)\n",
    "    y = tf.keras.layers.LeakyReLU(alpha=0.1)(y)\n",
    "    y = tf.keras.layers.Dropout(0.5)(y)\n",
    "    y = tf.keras.layers.Dense(1, activation = 'sigmoid')(y)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = x, outputs = y)\n",
    "    model.compile(loss='binary_crossentropy',optimizer = tf.keras.optimizers.Adam(lr=1e-5))\n",
    "    #model.summary()\n",
    "    return model\n",
    "df_model=define_model()\n",
    "# df_model.load_weights('/home/aelbakry1999/meso-pretrain/MesoInception_DF')\n",
    "f2f_model=define_model()\n",
    "f2f_model.load_weights('/home/aelbakry1999/meso-pretrain/MesoInception_F2F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PRETRAIN=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 160, 160, 4)  16          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 160, 160, 4)  16          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 160, 160, 2)  8           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 160, 160, 1)  4           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 160, 160, 4)  148         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 160, 160, 4)  148         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 160, 160, 2)  38          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 160, 160, 11) 0           conv2d_32[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 160, 160, 11) 44          concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 80, 80, 11)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 80, 80, 4)    48          max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 80, 80, 4)    48          max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 80, 80, 2)    24          max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 80, 80, 2)    24          max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 80, 80, 4)    148         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 80, 80, 4)    148         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 80, 80, 2)    38          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 80, 80, 12)   0           conv2d_39[0][0]                  \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 80, 80, 12)   48          concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 40, 40, 12)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 40, 40, 16)   4816        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 40, 40, 16)   64          conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 20, 20, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 20, 20, 16)   6416        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 20, 16)   64          conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 10, 10, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1600)         0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1600)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           25616       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            17          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 37,941\n",
      "Trainable params: 37,831\n",
      "Non-trainable params: 110\n",
      "__________________________________________________________________________________________________\n",
      "Train on 128022 samples\n",
      "Epoch 1/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.5643\n",
      "Epoch 2/20\n",
      "128022/128022 [==============================] - 141s 1ms/sample - loss: 0.3526\n",
      "Epoch 3/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.3252\n",
      "Epoch 4/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.3125\n",
      "Epoch 5/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.3032\n",
      "Epoch 6/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2958\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2904\n",
      "Epoch 8/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2891\n",
      "Epoch 9/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2842\n",
      "Epoch 10/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2819\n",
      "Epoch 11/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2794\n",
      "Epoch 12/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2773\n",
      "Epoch 13/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2770\n",
      "Epoch 14/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2751\n",
      "Epoch 15/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2736\n",
      "Epoch 16/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2735\n",
      "Epoch 17/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2702\n",
      "Epoch 18/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2697\n",
      "Epoch 19/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2695\n",
      "Epoch 20/20\n",
      "128022/128022 [==============================] - 142s 1ms/sample - loss: 0.2683\n",
      "fold 0 model loss: 0.5786352492290643\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 128\n",
    "\n",
    "kfolds=5\n",
    "kf = KFold(n_splits=kfolds)\n",
    "kf.get_n_splits(X, y)\n",
    "\n",
    "epochs = 20\n",
    "losses=[]\n",
    "\n",
    "if LOAD_PRETRAIN:\n",
    "    # import keras.backend as K\n",
    "    df_models=[]\n",
    "    i=0\n",
    "\n",
    "    model=define_model((160,160,3))\n",
    "    if i==0:\n",
    "        model.summary()\n",
    "    #model.load_weights('../input/meso-pretrain/MesoInception_DF')\n",
    "    for new_layer, layer in zip(model.layers[1:-8], df_model.layers[1:-8]):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "\n",
    "#     X_fold = np.asarray([X[z] for z in train_index])\n",
    "#     y_fold = np.asarray([y[z] for z in train_index])\n",
    "\n",
    "#     val_X_fold = np.asarray([X[z] for z in test_index])\n",
    "#     val_y_fold = np.asarray([y[z] for z in test_index])\n",
    "\n",
    "    log_dir = \"logs/fit/mesonet/\" + f'batch_size={batch_size} lr={lr} frame_index{frame_index}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "    model.fit(np.asarray(X),np.asarray(y), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[tensorboard_callback])   \n",
    "\n",
    "    pred= tf.math.sigmoid(model.predict(np.asarray(val_X)))\n",
    "    \n",
    "    loss=log_loss(val_y, pred)\n",
    "\n",
    "    losses.append(loss)\n",
    "    print('fold '+str(i)+' model loss: '+str(loss))\n",
    "    df_models.append(model)\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    i+=1\n",
    "        \n",
    "        \n",
    "#     i=0\n",
    "#     f2f_models=[]\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         model=define_model((160,160,3))\n",
    "#         #model.load_weights('../input/meso-pretrain/MesoInception_DF')\n",
    "#         for new_layer, layer in zip(model.layers[1:-8], f2f_model.layers[1:-8]):\n",
    "#             new_layer.set_weights(layer.get_weights())\n",
    "            \n",
    "#         X_fold = [X[z] for z in train_index]\n",
    "#         y_fold = [y[z] for z in train_index]\n",
    "        \n",
    "#         val_X_fold = [X[z] for z in test_index]\n",
    "#         val_y_fold = [y[z] for z in test_index]\n",
    "        \n",
    "#         model.fit([X_fold],[y_fold],epochs=epochs,callbacks=[learning_rate_reduction])\n",
    "#         pred=model.predict([val_X_fold])\n",
    "#         loss=log_loss(val_y_fold,pred)\n",
    "#         losses.append(loss)\n",
    "#         print('fold '+str(i)+' model loss: '+str(loss))\n",
    "#         f2f_models.append(model)\n",
    "#         K.clear_session()\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         i+=1\n",
    "#         models=f2f_models+df_models\n",
    "else:\n",
    "    models=[]\n",
    "    i=0\n",
    "    while len(models)<kfolds:\n",
    "        model=define_model((160,160,3))\n",
    "        if i==0:\n",
    "            model.summary()\n",
    "        model.fit([X],[y],epochs=epochs,callbacks=[learning_rate_reduction])\n",
    "        pred=model.predict([val_X])\n",
    "        loss=log_loss(val_y,pred)\n",
    "        losses.append(loss)\n",
    "        print('fold '+str(i)+' model loss: '+str(loss))\n",
    "        if loss<0.68:\n",
    "            models.append(model)\n",
    "        else:\n",
    "            print('loss too bad, retrain!')\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_pipline(X,two_times=False):\n",
    "    preds=[]\n",
    "    for model in tqdm(models):\n",
    "        pred=model.predict([X])\n",
    "        preds.append(pred)\n",
    "    preds=sum(preds)/len(preds)\n",
    "    if two_times:\n",
    "        return larger_range(preds,2)\n",
    "    else:\n",
    "        return preds\n",
    "def larger_range(model_pred,time):\n",
    "    return (((model_pred-0.5)*time)+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pred=models[losses.index(min(losses))].predict([val_X])\n",
    "model_pred=prediction_pipline(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(val_y, model_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(val_y, model_pred.round()).ravel()\n",
    "\n",
    "\n",
    "print(\"-------------- Confusion Matrix -------------- \")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "print('True Positives: {}, True Negatives: {}, False Positives: {}, False Negatives: {}'.format(tp, tn, fp, fn))\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2*(precision*recall)/ (precision+recall)\n",
    "print(\"-------------- Model Scores -------------- \")\n",
    "print('Precision: {}, Accuracy: {}, Recall: {}, F1-score: {}'.format(precision, accuracy, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answers(pred,real,num):\n",
    "    for i,(x,y) in enumerate(zip(pred,real)):\n",
    "        correct_incorrect='correct ✅ ' if round(float(x),0)==round(float(y),0) else 'incorrect❌'\n",
    "        print(correct_incorrect+' prediction: '+str(x[0])+', answer: '+str(y))\n",
    "        if i>num:\n",
    "            return\n",
    "def correct_precentile(pred,real):\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for x,y in zip(pred,real):\n",
    "        if round(float(x),0)==round(float(y),0):\n",
    "            correct+=1\n",
    "        else:\n",
    "            incorrect+=1\n",
    "    print('number correct: '+str(correct)+', number incorrect: '+str(incorrect))\n",
    "    print(str(round(correct/len(real)*100,1))+'% correct'+', '+str(round(incorrect/len(real)*100,1))+'% incorrect')\n",
    "check_answers(model_pred,val_y,15)\n",
    "correct_precentile(model_pred,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = 48\n",
    "\n",
    "path, labels = load_data(df_index, df_test_all[df_index-47])\n",
    "\n",
    "Xy_index = random.randint(0,len(labels))\n",
    "Xy_index = 100\n",
    "\n",
    "test_video_path, test_video_label = path[Xy_index], labels[Xy_index]\n",
    "test_video = read_img(test_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(test_video_label)\n",
    "plt.imshow(test_video[4])\n",
    "test_video_label\n",
    "\n",
    "test_video_embed = embed([test_video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = prediction_pipline(test_video[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
